# -*- coding: utf-8 -*-
"""DLS_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qa0e1EYPlY4QE4xmb8_-rxxVNX2_DrQ8

# Some installations
"""

# !pip install -q mediapipe googletrans==3.1.0a0
#
# !wget -O classifier.tflite -q https://storage.googleapis.com/mediapipe-models/image_classifier/efficientnet_lite2/float32/1/efficientnet_lite2.tflite
#
# !wget -O model.tflite -q https://storage.googleapis.com/mediapipe-models/interactive_segmenter/magic_touch/float32/1/magic_touch.tflite
#
# !pip install -q wget gensim
#
# !pip install -q spacy
#
# !python -m spacy download ru_core_news_md > /dev/null 2>&1

"""# Uploading images
If they are in the computer's memory
"""
def unpload_photo_to_recognize():
    from google.colab import files
    uploaded = files.upload()

    for filename in uploaded:
      content = uploaded[filename]
      with open(filename, 'wb') as f:
        f.write(content)
    IMAGE_FILENAMES = list(uploaded.keys())

    print('Uploaded files:', IMAGE_FILENAMES)

"""# MediaPipe and Google Translate"""

def _normalized_to_pixel_coordinates(
    normalized_x: float, normalized_y: float, image_width: int,
    image_height: int):
  """Converts normalized value pair to pixel coordinates."""

  # Checks if the float value is between 0 and 1.
  def is_valid_normalized_value(value: float) -> bool:
    return (value > 0 or math.isclose(0, value)) and (value < 1 or
                                                      math.isclose(1, value))

  if not (is_valid_normalized_value(normalized_x) and
          is_valid_normalized_value(normalized_y)):
    # TODO: Draw coordinates even if it's outside of the image bounds.
    return None
  x_px = min(math.floor(normalized_x * image_width), image_width - 1)
  y_px = min(math.floor(normalized_y * image_height), image_height - 1)
  return x_px, y_px

# Performs resizing and showing the image
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)

def image_preprocessing():
    import cv2
    from google.colab.patches import cv2_imshow
    import math
    import numpy as np
    import mediapipe as mp
    from mediapipe.tasks import python
    from mediapipe.tasks.python import vision
    from mediapipe.tasks.python.components import containers

    # Height and width that will be used by the model
    DESIRED_HEIGHT = 480
    DESIRED_WIDTH = 480

    # Preview the image(s)
    images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
    for name, image in images.items():
      print(name)
      resize_and_show(image)

    x = 0.7 #@param {type:"slider", min:0, max:1, step:0.01}
    y = 0.68 #@param {type:"slider", min:0, max:1, step:0.01}

    BG_COLOR = (192, 192, 192) # gray
    MASK_COLOR = (255, 255, 255) # white

    RegionOfInterest = vision.InteractiveSegmenterRegionOfInterest
    NormalizedKeypoint = containers.keypoint.NormalizedKeypoint

    # Create the options that will be used for InteractiveSegmenter
    base_options = python.BaseOptions(model_asset_path='model.tflite')
    options = vision.ImageSegmenterOptions(base_options=base_options,
                                           output_category_mask=True)

    # Create the interactive segmenter
    with vision.InteractiveSegmenter.create_from_options(options) as segmenter:

      # Loop through demo image(s)
      for image_file_name in IMAGE_FILENAMES:

        # Create the MediaPipe image file that will be segmented
        image = mp.Image.create_from_file(image_file_name)

        # Retrieve the masks for the segmented image
        roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,
                               keypoint=NormalizedKeypoint(x, y))
        segmentation_result = segmenter.segment(image, roi)
        category_mask = segmentation_result.category_mask

        # Generate solid color images for showing the output segmentation mask.
        image_data = image.numpy_view()
        fg_image = np.zeros(image_data.shape, dtype=np.uint8)
        fg_image[:] = MASK_COLOR
        bg_image = np.zeros(image_data.shape, dtype=np.uint8)
        bg_image[:] = BG_COLOR

        condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1
        output_image = np.where(condition, fg_image, bg_image)

        # Draw a white dot with black border to denote the point of interest
        thickness, radius = 6, -1
        keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)
        cv2.circle(output_image, keypoint_px, thickness + 5, (0, 0, 0), radius)
        cv2.circle(output_image, keypoint_px, thickness, (255, 255, 255), radius)

        print(f'Segmentation mask of {image_file_name}:')
        resize_and_show(output_image)

    # Blur the image background based on the segmentation mask.

    # Create the segmenter
    with python.vision.InteractiveSegmenter.create_from_options(options) as segmenter:

      # Loop through available image(s)
      for image_file_name in IMAGE_FILENAMES:

        # Create the MediaPipe Image
        image = mp.Image.create_from_file(image_file_name)

        # Retrieve the category masks for the image
        roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,
                               keypoint=NormalizedKeypoint(x, y))
        segmentation_result = segmenter.segment(image, roi)
        category_mask = segmentation_result.category_mask

        # Convert the BGR image to RGB
        image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)

        # Apply effects
        blurred_image = cv2.GaussianBlur(image_data, (55,55), 0)
        condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1
        output_image = np.where(condition, image_data, blurred_image)

        # Draw a white dot with black border to denote the point of interest
        thickness, radius = 6, -1
        keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)
        cv2.circle(output_image, keypoint_px, thickness + 5, (0, 0, 0), radius)
        cv2.circle(output_image, keypoint_px, thickness, (255, 255, 255), radius)

        print(f'Blurred background of {image_file_name}:')
        resize_and_show(output_image)

    OVERLAY_COLOR = (100, 100, 0) # cyan
    new_names=[]

    # Create the segmenter
    with python.vision.InteractiveSegmenter.create_from_options(options) as segmenter:

      # Loop through available image(s)
      for i, image_file_name in enumerate(IMAGE_FILENAMES):

        # Create the MediaPipe Image
        image = mp.Image.create_from_file(image_file_name)

        # Retrieve the category masks for the image
        roi = RegionOfInterest(format=RegionOfInterest.Format.KEYPOINT,
                               keypoint=NormalizedKeypoint(x, y))
        segmentation_result = segmenter.segment(image, roi)
        category_mask = segmentation_result.category_mask

        # Convert the BGR image to RGB
        image_data = cv2.cvtColor(image.numpy_view(), cv2.COLOR_BGR2RGB)

        # Create an overlay image with the desired color (e.g., (255, 0, 0) for red)
        overlay_image = np.zeros(image_data.shape, dtype=np.uint8)
        overlay_image[:] = OVERLAY_COLOR

        # Create the condition from the category_masks array
        alpha = np.stack((category_mask.numpy_view(),) * 3, axis=-1) > 0.1

        # Create an alpha channel from the condition with the desired opacity (e.g., 0.7 for 70%)
        alpha = alpha.astype(float) * 0.7

        # Blend the original image and the overlay image based on the alpha channel
        output_image = image_data * (1 - alpha) + overlay_image * alpha
        output_image = output_image.astype(np.uint8)

        # Draw a white dot with black border to denote the point of interest
        thickness, radius = 6, -1
        keypoint_px = _normalized_to_pixel_coordinates(x, y, image.width, image.height)
        cv2.circle(output_image, keypoint_px, thickness + 5, (0, 0, 0), radius)
        cv2.circle(output_image, keypoint_px, thickness, (255, 255, 255), radius)

        print(f'{image_file_name}:')
        resize_and_show(output_image)
        name='modified'+str(i)+'.jpg'
        new_names.append(name)
        cv2.imwrite(name, output_image)
    IMAGE_FILENAMES=new_names

def display_one_image(image, title, subplot, titlesize=16):
    """Displays one image along with the predicted category name and score."""
    plt.subplot(*subplot)
    plt.imshow(image)
    if len(title) > 0:
        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))
    return (subplot[0], subplot[1], subplot[2]+1)

def display_batch_of_images(images, predictions):
    """Displays a batch of images with the classifications."""
    # Images and predictions.
    images = [image.numpy_view() for image in images]

    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.
    rows = int(math.sqrt(len(images)))
    cols = len(images) // rows

    # Size and spacing.
    FIGSIZE = 5.0
    SPACING = 0.1
    subplot=(rows,cols, 1)
    if rows < cols:
        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))
    else:
        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))

    # Display.
    for i, (image, prediction) in enumerate(zip(images[:rows*cols], predictions[:rows*cols])):
        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3
        subplot = display_one_image(image, prediction, subplot, titlesize=dynamic_titlesize)

    # Layout.
    plt.tight_layout()
    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)
    plt.show()


def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)


def recognition():
    # We implemented some functions to visualize the image classification results.
    from matplotlib import pyplot as plt
    plt.rcParams.update({
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.spines.left': False,
        'axes.spines.bottom': False,
        'xtick.labelbottom': False,
        'xtick.bottom': False,
        'ytick.labelleft': False,
        'ytick.left': False,
        'xtick.labeltop': False,
        'xtick.top': False,
        'ytick.labelright': False,
        'ytick.right': False
    })

    import cv2
    from google.colab.patches import cv2_imshow
    import math

    DESIRED_HEIGHT = 480
    DESIRED_WIDTH = 480



    # Preview the images.
    images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}
    for name, image in images.items():
      print(name)
      resize_and_show(image)

    # STEP 1: Import the necessary modules.
    import mediapipe as mp
    from mediapipe.tasks import python
    from mediapipe.tasks.python.components import processors
    from mediapipe.tasks.python import vision

    # STEP 2: Create an ImageClassifier object.
    base_options = python.BaseOptions(model_asset_path='classifier.tflite')
    options = vision.ImageClassifierOptions(
        base_options=base_options, max_results=4)
    classifier = vision.ImageClassifier.create_from_options(options)

    images = []
    predictions = []
    for image_name in IMAGE_FILENAMES:
      # STEP 3: Load the input image.
      image = mp.Image.create_from_file(image_name)

      # STEP 4: Classify the input image.
      classification_result = classifier.classify(image)

      # STEP 5: Process the classification result. In this case, visualize it.
      images.append(image)
      top_category = classification_result.classifications[0].categories[0]
      predictions.append(f"{top_category.category_name} ({top_category.score:.2f})")

    display_batch_of_images(images, predictions)

"""# Verification and preprocessing of recognized ingredients"""
def verification():
    # display_batch_of_images(images, predictions)

    # @title Verification of recognized ingredients
    names_pred=[i[:-7] for i in predictions]

    # @markdown Is there any mistakes in ingredients recognition?
    answer="yes" #@param ["yes", "no"]

    # @title Correcting incorrectly recognized ingredients
    from ipywidgets import *
    if answer=="yes":
      tab_contents = [i for i in names_pred]
      ingredients = [widgets.VBox([widgets.Image(value=open(IMAGE_FILENAMES[i], "rb").read(), format='png', width=200, height=200), widgets.Text(value=f"If it's not {name}, write the correct name below", disabled=True), widgets.Text(description='Right name')]) for i, name in enumerate(tab_contents)]
      tab = widgets.Tab()
      tab.children = ingredients
      for i in range(len(ingredients)):
        tab.set_title(i, 'Photo '+str(i))
      display(tab)

    """# Preproccessing of predicted ingredients"""

    key="disabled=True), Text(value='"
    end_key="', description='Right name'"
    string=str(tab.children)
    preverified_predictions=[]

    for i in range(len(ingredients)):
      start=string.find(key)+len(key)
      end=string.find(end_key)
      preverified_predictions.append(string[start:end])
      string=string[end+len(end_key):]

    import numpy as np
    from dataclasses import dataclass

    @dataclass
    class SearchResults:
        relevance_scores: np.array


    # Methods for each picked ranking metrics
    class Ranking:
      def __init__(self, search_results: SearchResults):
        self.search_results = search_results
        self.relevance_scores = search_results.relevance_scores

      def accuracy(self, k):
        return np.sum(np.array(self.relevance_scores))/k

      def precision(self, k) -> float:
        relevant_results = np.array(self.relevance_scores)[:k]
        return np.mean(relevant_results)

      def recall(self, k):
        relevant_docs = np.sum(self.relevance_scores[:k])
        total_relevant_docs = np.sum(self.relevance_scores)
        return relevant_docs / total_relevant_docs
      def f1(self, k):
        return 2*self.precision(k)*self.recall(k)/(self.precision(k)+self.recall(k))

    true_counter=[1 if i=='' else 0 for i in preverified_predictions]

    relevant_results=SearchResults(true_counter)
    ranked=Ranking(relevant_results)
    k=len(true_counter)
    print(f"Accuracy@{k}: {ranked.accuracy(k)}")
    print(f"Precision@{k}: {ranked.precision(k)}")
    print(f"Recall@{k}: {ranked.recall(k)}")
    print(f"F1@{k}: {ranked.f1(k)}")

    from googletrans import Translator

    verified_predictions=[names_pred[i].lower() if name=='' else name.lower() for i, name in enumerate(preverified_predictions)]

    # init the Google API translator
    translator = Translator()

    # translate a russian text to english text
    predictions_rus=translator.translate(str(verified_predictions), dest='ru').text
    predictions_rus=predictions_rus[2:-2].split("', '")
    print(predictions_rus)